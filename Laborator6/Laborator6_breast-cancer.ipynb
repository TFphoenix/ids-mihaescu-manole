{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bitidsconda6aac95af6de54c59bceda5e4272aaa84",
   "display_name": "Python 3.7.6 64-bit ('IDS': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nume studenti:\n",
    "- Alexandra Manole\n",
    "- Teodor Mihaescu\n",
    "\n",
    "## Grupa: 382"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Set 1: Breast Cancer\n",
    "### (Missing values: yes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer  \n",
    "from sklearn.impute import IterativeImputer, SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_validate, cross_val_score, GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 286 entries, 0 to 285\nData columns (total 10 columns):\n #   Column      Non-Null Count  Dtype \n---  ------      --------------  ----- \n 0   Class       286 non-null    object\n 1   Age         286 non-null    object\n 2   Menopause   286 non-null    object\n 3   TumorSize   286 non-null    object\n 4   InvNodes    286 non-null    object\n 5   NodeCaps    278 non-null    object\n 6   DegMalig    286 non-null    int64 \n 7   Breast      286 non-null    object\n 8   BreastQuad  285 non-null    object\n 9   Irradiat    286 non-null    object\ndtypes: int64(1), object(9)\nmemory usage: 22.5+ KB\n"
    }
   ],
   "source": [
    "# Citeste datele\n",
    "data = pd.read_csv('./Datasets/breast-cancer.csv')\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prelucreaza si scaleaza datele\n",
    "def scale_category(name, max_value):\n",
    "    category_splitted = np.char.split(np.array(data[name], dtype=str), sep='-')\n",
    "    category_splitted = np.array([x for x in category_splitted], dtype=int)\n",
    "    data[name] = np.mean(category_splitted, axis=1) / max_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "     Class       Age  Menopause  TumorSize  InvNodes NodeCaps  DegMalig  \\\n0        0  0.348485        0.0   0.542373  0.025641        0       1.0   \n1        0  0.449495        0.0   0.372881  0.025641        0       0.5   \n2        0  0.449495        0.0   0.372881  0.025641        0       0.5   \n3        0  0.651515        1.0   0.288136  0.025641        0       0.5   \n4        0  0.449495        0.0   0.033898  0.025641        0       0.5   \n..     ...       ...        ...        ...       ...      ...       ...   \n281      1  0.348485        0.0   0.542373  0.025641        0       0.5   \n282      1  0.348485        0.0   0.372881  0.025641        0       1.0   \n283      1  0.651515        1.0   0.372881  0.025641        0       0.0   \n284      1  0.449495        1.0   0.542373  0.102564        0       1.0   \n285      1  0.550505        1.0   0.542373  0.102564        0       1.0   \n\n     Breast BreastQuad  Irradiat  \n0         0          0         0  \n1         1       0.75         0  \n2         0          0         0  \n3         1       0.25         0  \n4         1        0.5         0  \n..      ...        ...       ...  \n281       0       0.25         0  \n282       0       0.25         1  \n283       1       0.25         0  \n284       0          0         0  \n285       0          0         0  \n\n[286 rows x 10 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Class</th>\n      <th>Age</th>\n      <th>Menopause</th>\n      <th>TumorSize</th>\n      <th>InvNodes</th>\n      <th>NodeCaps</th>\n      <th>DegMalig</th>\n      <th>Breast</th>\n      <th>BreastQuad</th>\n      <th>Irradiat</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>0.348485</td>\n      <td>0.0</td>\n      <td>0.542373</td>\n      <td>0.025641</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0.449495</td>\n      <td>0.0</td>\n      <td>0.372881</td>\n      <td>0.025641</td>\n      <td>0</td>\n      <td>0.5</td>\n      <td>1</td>\n      <td>0.75</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>0.449495</td>\n      <td>0.0</td>\n      <td>0.372881</td>\n      <td>0.025641</td>\n      <td>0</td>\n      <td>0.5</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>0.651515</td>\n      <td>1.0</td>\n      <td>0.288136</td>\n      <td>0.025641</td>\n      <td>0</td>\n      <td>0.5</td>\n      <td>1</td>\n      <td>0.25</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0.449495</td>\n      <td>0.0</td>\n      <td>0.033898</td>\n      <td>0.025641</td>\n      <td>0</td>\n      <td>0.5</td>\n      <td>1</td>\n      <td>0.5</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>281</th>\n      <td>1</td>\n      <td>0.348485</td>\n      <td>0.0</td>\n      <td>0.542373</td>\n      <td>0.025641</td>\n      <td>0</td>\n      <td>0.5</td>\n      <td>0</td>\n      <td>0.25</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>282</th>\n      <td>1</td>\n      <td>0.348485</td>\n      <td>0.0</td>\n      <td>0.372881</td>\n      <td>0.025641</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0.25</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>283</th>\n      <td>1</td>\n      <td>0.651515</td>\n      <td>1.0</td>\n      <td>0.372881</td>\n      <td>0.025641</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>1</td>\n      <td>0.25</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>284</th>\n      <td>1</td>\n      <td>0.449495</td>\n      <td>1.0</td>\n      <td>0.542373</td>\n      <td>0.102564</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>285</th>\n      <td>1</td>\n      <td>0.550505</td>\n      <td>1.0</td>\n      <td>0.542373</td>\n      <td>0.102564</td>\n      <td>0</td>\n      <td>1.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>286 rows Ã— 10 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "# Class\n",
    "data['Class'] = np.where(data['Class'] == 'no-recurrence-events', 0, 1)\n",
    "\n",
    "# Age (10-99)\n",
    "scale_category('Age', 99)\n",
    "\n",
    "# Menopause\n",
    "data['Menopause'] = np.where(data['Menopause'] == 'premeno', 0.0, data['Menopause'])\n",
    "data['Menopause'] = np.where(data['Menopause'] == 'lt40', 0.5, data['Menopause'])\n",
    "data['Menopause'] = np.where(data['Menopause'] == 'ge40', 1.0, data['Menopause'])\n",
    "data['Menopause'] = data['Menopause'].astype('float64')\n",
    "\n",
    "# Tumor Size (0-59)\n",
    "scale_category('TumorSize',59)\n",
    "\n",
    "# Involved Nodes(0-39)\n",
    "scale_category('InvNodes', 39)\n",
    "\n",
    "# Node Caps\n",
    "data['NodeCaps'] = np.where(data['NodeCaps'] == 'yes', 1, data['NodeCaps'])\n",
    "data['NodeCaps'] = np.where(data['NodeCaps'] == 'no', 0, data['NodeCaps'])\n",
    "# data['NodeCaps'] = data['NodeCaps'].astype('float64')\n",
    "\n",
    "# Deg Malig\n",
    "data['DegMalig'] = data['DegMalig'] / 2 - 0.5\n",
    "\n",
    "# Breast\n",
    "data['Breast'] = np.where(data['Breast'] == 'left', 0, 1)\n",
    "\n",
    "# Breast Quad\n",
    "values = np.linspace(0, 1, 5)\n",
    "data['BreastQuad'] = np.where(data['BreastQuad'] == 'left_low', values[0], data['BreastQuad'])\n",
    "data['BreastQuad'] = np.where(data['BreastQuad'] == 'left_up', values[1], data['BreastQuad'])\n",
    "data['BreastQuad'] = np.where(data['BreastQuad'] == 'right_low', values[2], data['BreastQuad'])\n",
    "data['BreastQuad'] = np.where(data['BreastQuad'] == 'right_up', values[3], data['BreastQuad'])\n",
    "data['BreastQuad'] = np.where(data['BreastQuad'] == 'central', values[4], data['BreastQuad'])\n",
    "# data['BreastQuad'] = data['BreastQuad'].astype('float64')\n",
    "\n",
    "# Irradiat\n",
    "data['Irradiat'] = np.where(data['Irradiat'] == 'yes', 1, 0)\n",
    "\n",
    "# pd.set_option(\"display.max_rows\", 20, \"display.max_columns\", 20)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 286 entries, 0 to 285\nData columns (total 10 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   Class       286 non-null    int32  \n 1   Age         286 non-null    float64\n 2   Menopause   286 non-null    float64\n 3   TumorSize   286 non-null    float64\n 4   InvNodes    286 non-null    float64\n 5   NodeCaps    278 non-null    object \n 6   DegMalig    286 non-null    float64\n 7   Breast      286 non-null    int32  \n 8   BreastQuad  285 non-null    object \n 9   Irradiat    286 non-null    int32  \ndtypes: float64(5), int32(3), object(2)\nmemory usage: 19.1+ KB\n"
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values imputation\n",
    "\"\"\"\n",
    "Observam ca avem urmatoarele missing values:\n",
    "- 1 x BreastQuad\n",
    "- 8 x NodeCaps\n",
    "\n",
    "Pentru BreastQuad vom folosi un fillna dupa frecventa, ce va inlocui valoare lipsa cu cea mai itnalnita valoare, fapt care nu va influenta prea mult datele noaste. Facem acest lucru deoarece nu exista o relatie prea sigura intre localizarea cancerului mamar si celelalte atribute\n",
    "\n",
    "Pentru NodeCaps vom folosi un IterativeImputer deoarece acest atribut are legatura cu majoritatea celorlalte atribute, acesta fiind un multivariate imputer. Desi acest lucru este mai costisitor din cauza clasificarii facute de acesta. (initial_strategy='most_frequent' deoarece avem doar yes si no, nu vrem medie, mediana sau constanta)\n",
    "\"\"\"\n",
    "data['BreastQuad'].fillna(data['BreastQuad'].mode()[0], inplace=True)\n",
    "\n",
    "iterativeImputer = IterativeImputer(random_state=0, initial_strategy='most_frequent')\n",
    "data_columns = data.columns\n",
    "data_index = data.index\n",
    "data = pd.DataFrame(iterativeImputer.fit_transform(data))\n",
    "data.columns = data_columns\n",
    "data.index = data_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 286 entries, 0 to 285\nData columns (total 10 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   Class       286 non-null    float64\n 1   Age         286 non-null    float64\n 2   Menopause   286 non-null    float64\n 3   TumorSize   286 non-null    float64\n 4   InvNodes    286 non-null    float64\n 5   NodeCaps    286 non-null    float64\n 6   DegMalig    286 non-null    float64\n 7   Breast      286 non-null    float64\n 8   BreastQuad  286 non-null    float64\n 9   Irradiat    286 non-null    float64\ndtypes: float64(10)\nmemory usage: 22.5 KB\n"
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separarea setului de date\n",
    "X = data.loc[:, 'Age':]\n",
    "y = data.loc[:, :'Class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/3, shuffle=True)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "X_train_mat = X_train.to_numpy()\n",
    "X_test_mat = X_test.to_numpy()\n",
    "y_train_col = y_train.to_numpy()\n",
    "y_test_col = y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Counter({0.0: 138, 1.0: 52})\nCounter({0.0: 63, 1.0: 33})\n"
    }
   ],
   "source": [
    "# Verifica daca sunt egal distribuite\n",
    "print(Counter(y_train_col[:,0]))\n",
    "print(Counter(y_test_col[:,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "\"\"\"\n",
    "Principiul din spatele acestei metode este de a gasi un numar predefinit de cazuri apropiate ca distanta de noul punct care se incearca a fi prezis. Alegerea vecinilor influenteaza rezultatele modelului. Am observant ca in jurul valorii de 10 a hiperparametrului n_neighbors obtinem valori mai bune. \n",
    "Algoritmul necesita multe resurse la partea de testare deoarece parcurge datele \"memorate\" in intregime, pe cand cea de antrenare se intampla aproape instant (\"retinerea\" datelor).\n",
    "\"\"\"\n",
    "model1 = KNeighborsClassifier(n_neighbors=10)\n",
    "model1.fit(X_train_mat, y_train_col)\n",
    "y_hat = model1.predict(X_test_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Predicted:\t[0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n\nGround truth:\t[[1.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [1.]\n [1.]\n [1.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [1.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [1.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [1.]\n [0.]\n [1.]\n [1.]\n [0.]\n [1.]\n [0.]\n [1.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [1.]\n [1.]\n [1.]\n [0.]\n [0.]\n [0.]\n [1.]\n [1.]\n [0.]\n [0.]]\n\nFailed:\t[33 63 33 33 33 33 33 33 63 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33\n 33 33 33 33 33 63 33 33 33 33 33 63 33 33 33 33 33 33 33 33 33 33 33 33\n 33 63 33 63 33 33 33 33 33 33 33 33 33 33 33 63 33 33 33 33 33 33 33 33\n 33 63 33 33 63 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33]\n"
    }
   ],
   "source": [
    "print(f'Predicted:\\t{y_hat}\\n')\n",
    "print(f'Ground truth:\\t{y_test_col}\\n')\n",
    "print(f'Failed:\\t{sum(y_hat != y_test_col)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Mean Absolute Error:\t0.3333333333333333\nMean Squared Error:\t0.3333333333333333\nAccuracy:\t\t66.667%\n"
    }
   ],
   "source": [
    "print(f'Mean Absolute Error:\\t{metrics.mean_absolute_error(y_test_col, y_hat)}')\n",
    "print(f'Mean Squared Error:\\t{metrics.mean_squared_error(y_test_col, y_hat)}')\n",
    "print(f'Accuracy:\\t\\t{np.round(metrics.accuracy_score(y_test_col, y_hat) * 100, 3)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'fit_time': array([0.00598502, 0.00697494, 0.00298619, 0.00199509, 0.00448513]),\n 'score_time': array([0.01296544, 0.00598454, 0.00797939, 0.00897884, 0.0104003 ]),\n 'test_accuracy': array([0.70689655, 0.73684211, 0.77192982, 0.75438596, 0.66666667]),\n 'train_accuracy': array([0.76754386, 0.74672489, 0.74672489, 0.7510917 , 0.77292576]),\n 'test_f1': array([0.        , 0.21052632, 0.38095238, 0.5       , 0.42424242]),\n 'train_f1': array([0.48543689, 0.39583333, 0.34090909, 0.37362637, 0.43478261])}"
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "# Cross Validation\n",
    "results_train = cross_validate(model1, X, y, scoring=['accuracy','f1'], return_train_score=True)\n",
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cautare hiperparametrii optimi - GridSearch\n",
    "parameter_grid = {'n_neighbors': list(range(1, 20)), 'p': [1, 2, 3]}\n",
    "grid_search = GridSearchCV(estimator = KNeighborsClassifier(), param_grid=parameter_grid, scoring='accuracy', cv=4, return_train_score=True)\n",
    "grid_search.fit(X_train_mat, y_train_col)\n",
    "results_gscv = cross_val_score(grid_search, X_test_mat, y_test_col, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.6673684210526316\n{'n_neighbors': 6, 'p': 1}\n"
    }
   ],
   "source": [
    "print(results_gscv.mean())\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "\"\"\"\n",
    "Aceasta metoda folosita pentru clasificare poate lucra atat cu 2 clase (regresie binomiala), cat si cu o multitudine de clase (regresie multinomiala). Practic modelul dat de aceasta regresie determina probabilitatea ca obiectul determinat de vectorul de intrare sa apartina unei clase sau celeilalte.\n",
    "Acest model consta in determinarea unor ponderi corespondente datelor de intrare, astfel incat prezicerile datelor de iesire sa fie cat mai precise.\n",
    "\n",
    "In acest caz am folosit multi_class='ovr' deoarece avem de a face cu o regresie logistica binomiala.\n",
    "\"\"\"\n",
    "model2 = LogisticRegression(solver='liblinear', multi_class='ovr', max_iter=1000)\n",
    "model2.fit(X_train_mat, y_train_col)\n",
    "y_hat = model2.predict(X_test_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Predicted:\t[0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n\nGround truth:\t[[1.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [1.]\n [1.]\n [1.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [1.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [1.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [1.]\n [0.]\n [1.]\n [1.]\n [0.]\n [1.]\n [0.]\n [1.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [1.]\n [1.]\n [1.]\n [0.]\n [0.]\n [0.]\n [1.]\n [1.]\n [0.]\n [0.]]\n\nFailed:\t[33 33 33 33 33 33 33 33 63 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33\n 33 33 33 33 33 63 63 63 33 33 33 63 33 33 33 33 33 33 33 33 33 33 33 33\n 33 33 33 63 33 33 33 33 33 33 33 33 33 33 33 63 33 33 33 33 33 33 33 33\n 33 63 33 33 63 33 33 33 33 33 33 33 33 33 33 33 33 33 63 33 33 33 33 33]\n"
    }
   ],
   "source": [
    "print(f'Predicted:\\t{y_hat}\\n')\n",
    "print(f'Ground truth:\\t{y_test_col}\\n')\n",
    "print(f'Failed:\\t{sum(y_hat != y_test_col)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'fit_time': array([0.00199389, 0.00199246, 0.00099754, 0.00099492, 0.00099516]),\n 'score_time': array([0.0029943 , 0.0019927 , 0.00300002, 0.00199437, 0.00199533]),\n 'test_accuracy': array([0.81578947, 0.73684211, 0.71052632, 0.73684211, 0.71052632]),\n 'train_accuracy': array([0.73684211, 0.75      , 0.75      , 0.78947368, 0.76315789]),\n 'test_f1': array([0.58823529, 0.28571429, 0.26666667, 0.375     , 0.26666667]),\n 'train_f1': array([0.35483871, 0.40625   , 0.36666667, 0.48387097, 0.4       ])}"
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "# CV Train\n",
    "results_train = cross_validate(model2, X_train_mat, y_train_col, scoring=['accuracy','f1'], return_train_score=True)\n",
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'fit_time': array([0.0009973 , 0.0019958 , 0.00099516, 0.00099659, 0.00099921]),\n 'score_time': array([0.00299263, 0.00399232, 0.0019927 , 0.00399113, 0.00199032]),\n 'test_accuracy': array([0.7       , 0.68421053, 0.68421053, 0.84210526, 0.68421053]),\n 'train_accuracy': array([0.73684211, 0.74025974, 0.72727273, 0.68831169, 0.75324675]),\n 'test_f1': array([0.4       , 0.25      , 0.25      , 0.66666667, 0.25      ]),\n 'train_f1': array([0.44444444, 0.375     , 0.4       , 0.25      , 0.45714286])}"
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "# CV Test\n",
    "results_test = cross_validate(model2, X_test_mat, y_test_col, scoring=['accuracy','f1'], return_train_score=True)\n",
    "results_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cautare hiperparametrii optimi - GridSearch\n",
    "parameter_grid = {'max_iter': [100, 1000, 10000],\n",
    "    'C' : np.linspace(0.0, 2.0, 5),\n",
    "    'solver': ['saga', 'liblinear', 'lbfgs']}\n",
    "grid_search = GridSearchCV(estimator = LogisticRegression(), param_grid=parameter_grid, scoring='accuracy', cv=4, return_train_score=True)\n",
    "grid_search.fit(X_train_mat, y_train_col)\n",
    "results_gscv = cross_val_score(grid_search, X_test_mat, y_test_col, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.7189473684210526\n{'C': 1.5, 'max_iter': 100, 'solver': 'saga'}\n"
    }
   ],
   "source": [
    "print(results_gscv.mean())\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "\"\"\"\n",
    "Metoda aceasta este capabila sa faca clasificarea atat binomiala, cat si multinomiala. In clasificarea aceasta nodurile testeaza valoarea unui atribut anume, muchiile corespund raspunsului testului si fac legatura cu urmatoarele noduri, iar frunzele arborelui prezic rezultatul (efectiv incadrarea in clase).\n",
    "Modelul are in spate un arbore binar decizional, si ajunge la rezultate calculand asa numita \"impuritate\" (gini impurity), cu ajutorul careia determina cate split-uri sa faca (sau sa nu faca) la fiecare pas/nivel din structura decizionala.\n",
    "\"\"\"\n",
    "model3 = DecisionTreeClassifier(max_depth=2)\n",
    "model3.fit(X_train_mat, y_train_col)\n",
    "y_hat = model3.predict(X_test_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Predicted:\t[0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n\nGround truth:\t[[1.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [1.]\n [1.]\n [1.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [1.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [1.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [1.]\n [0.]\n [1.]\n [1.]\n [0.]\n [1.]\n [0.]\n [1.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [1.]\n [1.]\n [1.]\n [0.]\n [0.]\n [0.]\n [1.]\n [1.]\n [0.]\n [0.]]\n\nFailed:\t[33 33 33 33 33 63 33 33 63 33 33 33 33 33 33 33 33 33 33 33 33 33 33 63\n 33 33 33 33 33 63 63 33 33 33 33 33 33 33 33 33 33 33 63 33 33 33 33 33\n 33 63 33 63 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33\n 33 63 33 33 63 33 33 33 33 33 63 33 33 33 33 33 33 33 33 33 33 33 33 33]\n"
    }
   ],
   "source": [
    "print(f'Predicted:\\t{y_hat}\\n')\n",
    "print(f'Ground truth:\\t{y_test_col}\\n')\n",
    "print(f'Failed:\\t{sum(y_hat != y_test_col)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'fit_time': array([0.00199509, 0.0010016 , 0.00099111, 0.        , 0.00098729]),\n 'score_time': array([0.00299215, 0.00199032, 0.00100064, 0.00399542, 0.00299692]),\n 'test_accuracy': array([0.76315789, 0.68421053, 0.76315789, 0.78947368, 0.81578947]),\n 'train_accuracy': array([0.79605263, 0.76315789, 0.79605263, 0.78947368, 0.78289474]),\n 'test_f1': array([0.47058824, 0.        , 0.30769231, 0.5       , 0.58823529]),\n 'train_f1': array([0.49180328, 0.28      , 0.52307692, 0.48387097, 0.45901639])}"
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "# CV Train\n",
    "results_train = cross_validate(model3, X_train_mat, y_train_col, scoring=['accuracy','f1'], return_train_score=True)\n",
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'fit_time': array([0.00199652, 0.        , 0.00099754, 0.        , 0.00198698]),\n 'score_time': array([0.00299358, 0.00199461, 0.00099635, 0.00099611, 0.00299788]),\n 'test_accuracy': array([0.6       , 0.73684211, 0.57894737, 0.84210526, 0.57894737]),\n 'train_accuracy': array([0.76315789, 0.72727273, 0.75324675, 0.7012987 , 0.77922078]),\n 'test_f1': array([0.2       , 0.44444444, 0.33333333, 0.72727273, 0.33333333]),\n 'train_f1': array([0.52631579, 0.46153846, 0.59574468, 0.46511628, 0.62222222])}"
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "# CV Test\n",
    "results_test = cross_validate(model3, X_test_mat, y_test_col, scoring=['accuracy','f1'], return_train_score=True)\n",
    "results_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cautare hiperparametrii optimi - GridSearch\n",
    "parameter_grid = {'max_depth': list(range(3,7)),\n",
    "'min_samples_split': np.linspace(1,3,5),\n",
    "'min_samples_leaf': np.linspace(0.1,0.3,5),\n",
    "'max_features': ['sqrt', 'log2', None],\n",
    "'criterion': ['gini', 'entropy']}\n",
    "grid_search = GridSearchCV(estimator = DecisionTreeClassifier(), param_grid=parameter_grid, scoring='accuracy', cv=4, return_train_score=True)\n",
    "grid_search.fit(X_train_mat, y_train_col)\n",
    "results_gscv = cross_val_score(grid_search, X_test_mat, y_test_col, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.6557894736842105\n{'criterion': 'entropy', 'max_depth': 3, 'max_features': 'sqrt', 'min_samples_leaf': 0.15, 'min_samples_split': 1.0}\n"
    }
   ],
   "source": [
    "print(results_gscv.mean())\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4: RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "\"\"\"\n",
    "Aceasta metode de clasificare consta intr-o multitudine de arbori de decizie care lucreaza ca un intreg. Din fiecare arbore rezulta o clasa predictie si clasa cu cele mai multe \"voturi\" devine modelul de predictie.\n",
    "\"\"\"\n",
    "model4 = RandomForestClassifier(max_depth=2, random_state=0, ccp_alpha=0.2)\n",
    "model4.fit(X_train_mat, y_train_col)\n",
    "y_hat = model4.predict(X_test_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Predicted:\t[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n\nGround truth:\t[[1.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [1.]\n [1.]\n [1.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [1.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [1.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [1.]\n [0.]\n [1.]\n [1.]\n [0.]\n [1.]\n [0.]\n [1.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [1.]\n [1.]\n [1.]\n [0.]\n [0.]\n [0.]\n [1.]\n [1.]\n [0.]\n [0.]]\n\nFailed:\t[33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33\n 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33\n 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33\n 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33]\n"
    }
   ],
   "source": [
    "print(f'Predicted:\\t{y_hat}\\n')\n",
    "print(f'Ground truth:\\t{y_test_col}\\n')\n",
    "print(f'Failed:\\t{sum(y_hat != y_test_col)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'fit_time': array([0.40354657, 0.25647974, 0.20326734, 0.24032068, 0.20743847]),\n 'score_time': array([0.01795173, 0.01496124, 0.01097012, 0.00902677, 0.01197124]),\n 'test_accuracy': array([0.73684211, 0.73684211, 0.73684211, 0.71052632, 0.71052632]),\n 'train_accuracy': array([0.72368421, 0.72368421, 0.72368421, 0.73026316, 0.73026316]),\n 'test_f1': array([0., 0., 0., 0., 0.]),\n 'train_f1': array([0., 0., 0., 0., 0.])}"
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "# CV Train\n",
    "results_train = cross_validate(model4, X_train_mat, y_train_col, scoring=['accuracy','f1'], return_train_score=True)\n",
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'fit_time': array([0.24831939, 0.21742654, 0.20204449, 0.20246029, 0.20846248]),\n 'score_time': array([0.01097012, 0.01797557, 0.01495886, 0.01299977, 0.00997519]),\n 'test_accuracy': array([0.65      , 0.63157895, 0.63157895, 0.68421053, 0.68421053]),\n 'train_accuracy': array([0.65789474, 0.66233766, 0.66233766, 0.64935065, 0.64935065]),\n 'test_f1': array([0., 0., 0., 0., 0.]),\n 'train_f1': array([0., 0., 0., 0., 0.])}"
     },
     "metadata": {},
     "execution_count": 31
    }
   ],
   "source": [
    "# CV Test\n",
    "results_test = cross_validate(model4, X_test_mat, y_test_col, scoring=['accuracy','f1'], return_train_score=True)\n",
    "results_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cautare hiperparametrii optimi - GridSearch\n",
    "parameter_grid = {'max_depth': list(range(3,7)),\n",
    "'min_samples_split': list(range(1,4)),\n",
    "'min_samples_leaf': list(range(3,7))}\n",
    "grid_search = GridSearchCV(estimator = RandomForestClassifier(), param_grid=parameter_grid, scoring='accuracy', cv=4, return_train_score=True)\n",
    "grid_search.fit(X_train_mat, y_train_col)\n",
    "results_gscv = cross_val_score(grid_search, X_test_mat, y_test_col, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.6673684210526316\n{'max_depth': 3, 'min_samples_leaf': 5, 'min_samples_split': 2}\n"
    }
   ],
   "source": [
    "print(results_gscv.mean())\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 5: SVC (Support Vector Classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "model5 = SVC(C=2, gamma='auto')\n",
    "model5.fit(X_train_mat, y_train_col)\n",
    "y_hat = model5.predict(X_test_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Predicted:\t[0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1.\n 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n\nGround truth:\t[[1.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [1.]\n [1.]\n [1.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [1.]\n [1.]\n [1.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [1.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [1.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [1.]\n [0.]\n [1.]\n [0.]\n [1.]\n [1.]\n [0.]\n [1.]\n [0.]\n [1.]\n [0.]\n [1.]\n [0.]\n [0.]\n [0.]\n [1.]\n [1.]\n [1.]\n [0.]\n [0.]\n [0.]\n [1.]\n [1.]\n [0.]\n [0.]]\n\nFailed:\t[33 33 33 33 33 33 63 33 63 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33\n 33 33 33 33 33 63 63 33 33 33 33 63 33 33 33 33 33 33 33 33 33 33 33 33\n 33 63 33 63 33 33 33 33 33 33 33 33 33 33 33 63 33 33 33 33 33 33 33 63\n 33 63 33 33 63 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 33 63 33 33]\n"
    }
   ],
   "source": [
    "print(f'Predicted:\\t{y_hat}\\n')\n",
    "print(f'Ground truth:\\t{y_test_col}\\n')\n",
    "print(f'Failed:\\t{sum(y_hat != y_test_col)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'fit_time': array([0.00199199, 0.00199461, 0.00199533, 0.00099659, 0.00199413]),\n 'score_time': array([0.00299168, 0.00299215, 0.00099778, 0.00199485, 0.00199461]),\n 'test_accuracy': array([0.76315789, 0.76315789, 0.76315789, 0.78947368, 0.76315789]),\n 'train_accuracy': array([0.76973684, 0.76973684, 0.78289474, 0.77631579, 0.78289474]),\n 'test_f1': array([0.52631579, 0.30769231, 0.47058824, 0.42857143, 0.52631579]),\n 'train_f1': array([0.49275362, 0.46153846, 0.50746269, 0.37037037, 0.52173913])}"
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "# CV Train\n",
    "results_train = cross_validate(model5, X_train_mat, y_train_col, scoring=['accuracy','f1'], return_train_score=True)\n",
    "results_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'fit_time': array([0.00997066, 0.00698543, 0.00797963, 0.0029912 , 0.00598431]),\n 'score_time': array([0.00698447, 0.00398707, 0.00399041, 0.00498605, 0.00598741]),\n 'test_accuracy': array([0.70689655, 0.73684211, 0.70175439, 0.61403509, 0.63157895]),\n 'train_accuracy': array([0.77631579, 0.7510917 , 0.70305677, 0.74672489, 0.74235808]),\n 'test_f1': array([0.        , 0.21052632, 0.        , 0.42105263, 0.48780488]),\n 'train_f1': array([0.50485437, 0.31325301, 0.        , 0.45283019, 0.42718447])}"
     },
     "metadata": {},
     "execution_count": 37
    }
   ],
   "source": [
    "# Cross Validation\n",
    "results_test = cross_validate(model5, X, y, scoring=['accuracy','f1'], return_train_score=True)\n",
    "results_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cautare hiperparametrii optimi - GridSearch\n",
    "parameter_grid = {'C': np.linspace(0,1,5),\n",
    "'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "'gamma': ['scale', 'auto']}\n",
    "grid_search = GridSearchCV(estimator = SVC(), param_grid=parameter_grid, scoring='accuracy', cv=4, return_train_score=True)\n",
    "grid_search.fit(X_train_mat, y_train_col)\n",
    "results_gscv = cross_val_score(grid_search, X_test_mat, y_test_col, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "0.6768421052631579\n{'C': 0.75, 'gamma': 'scale', 'kernel': 'rbf'}\n"
    }
   ],
   "source": [
    "print(results_gscv.mean())\n",
    "print(grid_search.best_params_)"
   ]
  }
 ]
}